"""Plan node implementation for intelligent tool selection and planning."""

import re
import os
from typing import Dict, Any, List
from ts_agent.types import State, ToolType
from .schemas import PlanData, Plan, PlanRequest
from ts_agent.llm import planner_llm
from src.clients.prompts import get_prompt, PROMPT_NAMES
from src.utils.prompts import build_conversation_and_user_context, format_procedure_for_prompt
from src.utils.debug import dump_prompt_to_file, dump_response_to_file
from jsonschema import validate, ValidationError
import logging

logger = logging.getLogger(__name__)


def _validate_and_sanitize_plan(
    plan: Plan, 
    available_tools: List[Dict[str, Any]], 
    verified_email: str,
    conversation_id: str
) -> tuple[Plan, List[Dict[str, Any]]]:
    """
    Validate tool calls against tool schemas and sanitize parameters.
    
    Injects verified values from state:
    - user_email: Replaces any email parameter with verified email from Intercom
    - conversation_id: Injects conversation_id for action tools that need it
    
    NOTE: Action tools skip parameter validation/sanitization since Coverage 
    will generate parameters based on gathered data.
    
    Args:
        plan: Generated plan from LLM
        available_tools: List of available tools with schemas
        verified_email: Verified user email from Intercom (source of truth)
        conversation_id: Conversation ID from state
        
    Returns:
        Tuple of (validated_plan, validation_errors)
        - validated_plan: Plan with only valid tool calls
        - validation_errors: List of validation errors with details
    """
    # Create a lookup map for tools
    tools_map = {tool["name"]: tool for tool in available_tools}
    
    validated_tool_calls = []
    validation_errors = []
    
    for i, tool_call in enumerate(plan.tool_calls, 1):
        tool_name = tool_call.tool_name
        
        # 1. Validate tool exists
        if tool_name not in tools_map:
            available_names = ", ".join(tools_map.keys())
            error_msg = f"Tool '{tool_name}' not found. Available tools: {available_names}"
            logger.warning(f"âš ï¸  Tool call {i}: {error_msg}")
            print(f"   âš ï¸  Invalid tool: {tool_name} (not found)")
            validation_errors.append({
                "tool_call_index": i,
                "tool_name": tool_name,
                "parameters": tool_call.parameters,
                "reasoning": tool_call.reasoning,
                "error_type": "tool_not_found",
                "error_message": error_msg
            })
            continue
        
        tool_schema = tools_map[tool_name]
        input_schema = tool_schema.get("inputSchema", {})
        
        # Check if this is an action tool (skip parameter validation)
        tool_type = tool_schema.get("tool_type", ToolType.GATHER.value)
        is_action_tool = tool_type == ToolType.ACTION.value
        
        if is_action_tool:
            # Action tools skip validation - Coverage will generate parameters
            logger.info(f"âœ“ Action tool {tool_name}: skipping parameter validation (Coverage will handle)")
            print(f"   âœ“ Action tool {tool_name}: parameters will be generated by Coverage")
            
            # Keep the tool call as-is (just for reference, params will be ignored)
            from .schemas import ToolCall
            validated_tool_call = ToolCall(
                tool_name=tool_name,
                parameters={},  # Empty params - Coverage will generate
                reasoning=tool_call.reasoning
            )
            validated_tool_calls.append(validated_tool_call)
            continue
        
        # 2. Sanitize parameters (inject verified email, conversation_id, etc.)
        # Build injection map with trusted values
        import os
        from src.utils.sanitization import sanitize_tool_params
        
        injection_map = {
            "user_email": verified_email,
            "conversation_id": conversation_id,
            "dry_run": lambda: os.getenv("DRY_RUN", "false").lower() == "true",
        }
        
        try:
            sanitized_params = sanitize_tool_params(
                tool_call.parameters,
                input_schema,
                tool_name,
                injection_map
            )
        except Exception as e:
            error_msg = f"Parameter sanitization failed: {str(e)}"
            logger.warning(f"   âš ï¸  Tool {tool_name}: {error_msg}")
            print(f"   âš ï¸  Invalid params for {tool_name}: {e}")
            validation_errors.append({
                "tool_call_index": i,
                "tool_name": tool_name,
                "parameters": tool_call.parameters,
                "reasoning": tool_call.reasoning,
                "error_type": "sanitization_failed",
                "error_message": error_msg
            })
            continue
        
        # 3. Validate parameters against tool's input schema
        if input_schema and input_schema.get("properties"):
            try:
                validate(instance=sanitized_params, schema=input_schema)
            except ValidationError as e:
                error_msg = f"Parameter validation failed: {e.message}"
                logger.warning(f"âš ï¸  Tool call {i} ({tool_name}): {error_msg}")
                print(f"   âš ï¸  Invalid params for {tool_name}: {e.message}")
                validation_errors.append({
                    "tool_call_index": i,
                    "tool_name": tool_name,
                    "parameters": tool_call.parameters,
                    "reasoning": tool_call.reasoning,
                    "error_type": "schema_validation_failed",
                    "error_message": error_msg
                })
                continue
        
        # Create validated tool call with sanitized params
        from .schemas import ToolCall
        validated_tool_call = ToolCall(
            tool_name=tool_name,
            parameters=sanitized_params,
            reasoning=tool_call.reasoning
        )
        validated_tool_calls.append(validated_tool_call)
    
    # Log summary
    if validation_errors:
        logger.info(f"Validation complete: {len(validated_tool_calls)} valid, {len(validation_errors)} invalid")
    
    # Return new plan with validated tool calls and validation errors
    return Plan(
        reasoning=plan.reasoning,
        tool_calls=validated_tool_calls
    ), validation_errors




def plan_node(state: State) -> State:
    """
    Plan which tools to execute based on conversation history.
    
    This node analyzes the entire conversation history and creates a plan for which MCP tools
    to execute to best respond to the conversation.
    """
    # Get hops array and current hop number
    hops_array = state.get("hops", [])
    current_hop = len(hops_array)  # Current hop is the next one to be added
    
    # Create new hop data structure with nested fields
    hop_data = {
        "hop_number": current_hop + 1,
        "plan": None,
        "gather": None,
        "coverage": None
    }
    
    # Create plan request with available context from previous hops
    context = _build_context_from_hops(hops_array, state)
    
    # Add docs data to context
    docs_data = state.get("docs_data", {})
    if docs_data:
        context["available_docs"] = list(docs_data.keys())
    
    try:
        # Build formatted conversation history and user details (with validation)
        formatted_context = build_conversation_and_user_context(state)
        context["conversation_history_formatted"] = formatted_context["conversation_history"]
        context["user_details_formatted"] = formatted_context["user_details"]
    except ValueError as e:
        state["error"] = str(e)
        return state
    
    plan_request = PlanRequest(
        conversation_history=state.get("messages", []),
        user_email=None,  # No longer used, kept for schema compatibility
        context=context
    )
    
    try:
        # Get available tools from state
        available_tools = state.get("available_tools", [])
        
        # Get selected procedure from state (if any)
        selected_procedure = state.get("selected_procedure")
        
        # Get verified email from Intercom (source of truth)
        user_details = state.get("user_details", {})
        verified_email = user_details.get("email", "")
        
        # Get conversation_id from state
        conversation_id = state.get("conversation_id", "")
        
        # Generate plan using LLM (with retry logic for validation errors)
        max_retries = 1
        validation_errors = None
        previous_valid_plan = None
        
        for attempt in range(max_retries + 1):
            # Generate plan (pass validation errors from previous attempt if retrying)
            plan = _generate_plan(
                plan_request, 
                available_tools, 
                selected_procedure,
                validation_errors=validation_errors if attempt > 0 else None
            )
            
            # If this is a retry, merge the newly fixed tool calls with the previous valid ones
            if attempt > 0 and previous_valid_plan and validation_errors:
                logger.info(f"Merging {len(plan.tool_calls)} fixed tool calls with {len(previous_valid_plan.tool_calls)} valid tool calls from previous attempt")
                
                # Get indices of failed tool calls
                failed_indices = {err.get("tool_call_index") for err in validation_errors}
                
                # Rebuild the tool calls list: keep valid ones, replace failed ones
                from .schemas import ToolCall, Plan as PlanSchema
                merged_tool_calls = []
                
                # Track which fixed tool calls we've used (by tool name)
                fixed_tool_map = {tc.tool_name: tc for tc in plan.tool_calls}
                used_fixed_tools = set()
                
                # Iterate through original plan and merge
                for idx, original_tc in enumerate(previous_valid_plan.tool_calls, 1):
                    if idx in failed_indices:
                        # This was a failed call - replace with fixed version if available
                        if original_tc.tool_name in fixed_tool_map:
                            merged_tool_calls.append(fixed_tool_map[original_tc.tool_name])
                            used_fixed_tools.add(original_tc.tool_name)
                            logger.info(f"Replaced failed {original_tc.tool_name} with fixed version")
                        else:
                            logger.warning(f"No fixed version found for {original_tc.tool_name}, skipping")
                    else:
                        # This was valid - keep it
                        merged_tool_calls.append(original_tc)
                
                # Add any new fixed tools that weren't replacements
                for tool_name, fixed_tc in fixed_tool_map.items():
                    if tool_name not in used_fixed_tools:
                        merged_tool_calls.append(fixed_tc)
                        logger.info(f"Added new fixed {tool_name}")
                
                # Create merged plan
                plan = PlanSchema(
                    reasoning=f"{previous_valid_plan.reasoning} [Retry: Fixed {len(validation_errors)} tool call(s)]",
                    tool_calls=merged_tool_calls
                )
                logger.info(f"Merged plan has {len(merged_tool_calls)} total tool calls")
            
            # Validate and sanitize the plan (check tool schemas, inject verified parameters)
            validated_plan, validation_errors = _validate_and_sanitize_plan(
                plan, available_tools, verified_email, conversation_id
            )
            
            # If no validation errors, we're done
            if not validation_errors:
                if attempt > 0:
                    print(f"   âœ… Plan validation succeeded on retry {attempt}")
                break
            
            # If this was the last attempt, accept the plan with invalid calls dropped
            if attempt >= max_retries:
                print(f"   âš ï¸  Plan still has {len(validation_errors)} invalid tool calls after {max_retries} retry. Dropping them.")
                logger.warning(f"Plan validation failed after {max_retries} retry. Dropping {len(validation_errors)} invalid tool calls.")
                break
            
            # Otherwise, prepare for retry with validation errors
            # Save the original plan (with all tool calls) before retry
            previous_valid_plan = plan
            print(f"   ðŸ”„ Plan has {len(validation_errors)} invalid tool calls. Retrying with error details...")
            logger.info(f"Retrying plan generation (attempt {attempt + 2}/{max_retries + 1}) with validation errors")
        
        # Separate tool calls by type (gather vs action)
        gather_tool_calls = []
        action_tool_calls = []
        
        # Create lookup for tool types
        tools_type_map = {tool["name"]: tool.get("tool_type") for tool in available_tools}
        
        for tool_call in validated_plan.tool_calls:
            tool_type = tools_type_map.get(tool_call.tool_name, ToolType.GATHER.value)
            
            if tool_type == ToolType.ACTION.value:
                action_tool_calls.append(tool_call)
            else:
                gather_tool_calls.append(tool_call)
        
        # Store plan in nested structure using PlanData TypedDict
        # Convert ToolCall objects to dictionaries for state storage
        tool_calls_dicts = [tc.model_dump() for tc in validated_plan.tool_calls]
        gather_tool_calls_dicts = [tc.model_dump() for tc in gather_tool_calls]
        action_tool_calls_dicts = [tc.model_dump() for tc in action_tool_calls]
        
        plan_data: PlanData = {
            "plan": validated_plan.model_dump(),
            "tool_calls": tool_calls_dicts,  # All tools (backward compatibility)
            "gather_tool_calls": gather_tool_calls_dicts,  # Only gather tools
            "action_tool_calls": action_tool_calls_dicts,  # Only action tools
            "reasoning": validated_plan.reasoning,
        }
        hop_data["plan"] = plan_data
        
        print(f"ðŸ“‹ Plan generated (Hop {current_hop + 1}):")
        print(f"   ðŸ“Š Total: {len(validated_plan.tool_calls)} tools")
        print(f"   ðŸ” Gather tools: {len(gather_tool_calls)}")
        print(f"   âš¡ Action tools: {len(action_tool_calls)}")
        
        if gather_tool_calls:
            print(f"\n   Gather tools:")
            for i, tool_call in enumerate(gather_tool_calls, 1):
                print(f"      {i}. {tool_call.tool_name} - {tool_call.reasoning}")
        
        if action_tool_calls:
            print(f"\n   Action tools (for coverage to consider):")
            for i, tool_call in enumerate(action_tool_calls, 1):
                print(f"      {i}. {tool_call.tool_name} - {tool_call.reasoning}")
        
    except Exception as e:
        error_msg = f"Plan generation failed: {str(e)}"
        state["error"] = error_msg
        state["escalation_reason"] = error_msg
        state["next_node"] = "escalate"
        print(f"âŒ Plan generation error: {e}")
        return state
    
    # Add hop data to hops array
    state["hops"].append(hop_data)
    
    return state


def _generate_plan(
    request: PlanRequest, 
    available_tools: List[Dict[str, Any]], 
    selected_procedure: Dict[str, Any] = None,
    validation_errors: List[Dict[str, Any]] = None
) -> Plan:
    """
    Generate an execution plan using LLM.
    
    Args:
        request: Plan request with conversation history and context
        available_tools: List of available tools from MCP server
        selected_procedure: Optional selected procedure from RAG store
        validation_errors: Optional list of validation errors from previous attempt (for retry)
        
    Returns:
        Generated execution plan
    """
    
    # Create context-aware prompt for LLM
    context_info = _format_context_for_prompt(request.context)
    
    # Build conversation history and user details (structured format)
    # Get from context if available (passed from plan_node call)
    conversation_history = request.context.get("conversation_history_formatted", "")
    user_details = request.context.get("user_details_formatted", "")
    
    # Format procedure if available
    procedure_text = format_procedure_for_prompt(selected_procedure)
    
    # Format validation errors if this is a retry
    validation_errors_text = _format_validation_errors(validation_errors, available_tools) if validation_errors else ""
    
    # Get prompt from LangSmith
    prompt_template_text = get_prompt(PROMPT_NAMES["PLAN_NODE"])
    
    # Format the prompt with variables
    formatted_tools = _format_tools_for_prompt(available_tools)
    
    # Inject validation errors into the prompt if present
    if validation_errors_text:
        # For retry: Skip available_tools to save tokens - we already show the schema in validation errors
        prompt = prompt_template_text.format(
            conversation_history=conversation_history,
            user_details=user_details,
            procedure=procedure_text,
            context_info=context_info + "\n\n" + validation_errors_text,
            available_tools="(Tool schemas omitted for retry - see validation errors above for required parameters)"
        )
    else:
        # For first attempt: Include all available tools
        prompt = prompt_template_text.format(
            conversation_history=conversation_history,
            user_details=user_details,
            procedure=procedure_text,
            context_info=context_info,
            available_tools=formatted_tools
        )
    
    # Log the full prompt for debugging (only to logger, not console)
    logger.debug(f"Plan prompt length: {len(prompt)} characters")
    if validation_errors:
        logger.debug(f"Retry prompt with {len(validation_errors)} validation errors")
    
    # Debug: Dump full prompt to file if DEBUG_PROMPTS env var is set
    metadata = {
        "Prompt Length": f"{len(prompt)} characters",
        "Is Retry": bool(validation_errors),
        "Has Procedure": bool(selected_procedure)
    }
    if validation_errors:
        metadata["Validation Errors"] = len(validation_errors)
    
    dump_prompt_to_file(prompt, "plan", metadata=metadata)
        
    # Get LLM response with structured output
    # Use function_calling method since Plan schema contains Dict[str, Any] 
    # which is not supported by OpenAI's native structured output
    llm = planner_llm()
    llm_with_structure = llm.with_structured_output(Plan, method="function_calling")
    plan = llm_with_structure.invoke(prompt)
    
    # Debug: Dump LLM response to file if DEBUG_PROMPTS env var is set
    import time
    response_data = {
        "timestamp": time.strftime("%Y%m%d_%H%M%S"),
        "is_retry": bool(validation_errors),
        "reasoning": plan.reasoning,
        "tool_calls": [
            {
                "tool_name": tc.tool_name,
                "parameters": tc.parameters,
                "reasoning": tc.reasoning
            }
            for tc in plan.tool_calls
        ],
        "metadata": {
            "total_tool_calls": len(plan.tool_calls),
            "prompt_length": len(prompt),
            "has_procedure": bool(selected_procedure)
        }
    }
    
    suffix = "_retry" if validation_errors else "_response"
    dump_response_to_file(response_data, "plan", suffix=suffix)
    
    return plan


def _format_tools_for_prompt(tools: List[Dict[str, Any]]) -> str:
    """Format tools for LLM prompt with full schemas."""
    import json
    formatted = []
    
    for tool in tools:
        name = tool.get("name", "unknown")
        description = tool.get("description", "No description available")
        input_schema = tool.get("inputSchema", {})
        tool_type = tool.get("tool_type", "gather")  # Get tool type
        
        # Format tool with full schema and type
        tool_str = f"Tool: {name}\n"
        tool_str += f"Type: {tool_type}\n"  # Add type to prompt
        tool_str += f"Description: {description}\n"
        tool_str += f"Input Schema:\n{json.dumps(input_schema, indent=2)}"
        
        formatted.append(tool_str)
    
    return "\n\n".join(formatted)


def _format_validation_errors(
    validation_errors: List[Dict[str, Any]], 
    available_tools: List[Dict[str, Any]]
) -> str:
    """
    Format validation errors for the LLM prompt to help it fix the issues.
    Only asks the LLM to regenerate the failed tool calls.
    
    Args:
        validation_errors: List of validation error dictionaries
        available_tools: List of available tools with their schemas
        
    Returns:
        Formatted string describing validation errors
    """
    if not validation_errors:
        return ""
    
    # Create tool lookup map
    tools_map = {tool["name"]: tool for tool in available_tools}
    
    error_lines = [
        "=" * 80,
        "âš ï¸ VALIDATION ERRORS FROM PREVIOUS ATTEMPT",
        "=" * 80,
        f"The following {len(validation_errors)} tool call(s) had validation errors.",
        "Please regenerate ONLY these failed tool calls with the corrections.\n"
    ]
    
    for i, error in enumerate(validation_errors, 1):
        tool_name = error.get("tool_name", "unknown")
        error_type = error.get("error_type", "unknown")
        error_message = error.get("error_message", "")
        reasoning = error.get("reasoning", "")
        parameters = error.get("parameters", {})
        tool_call_index = error.get("tool_call_index", 0)
        
        error_lines.append(f"FAILED TOOL {i}: '{tool_name}'")
        error_lines.append(f"-" * 40)
        error_lines.append(f"Your Original Reasoning: {reasoning}")
        error_lines.append(f"Error: {error_message}")
        
        import json
        error_lines.append(f"Your Invalid Parameters: {json.dumps(parameters, indent=2)}")
        
        # Add the tool's required schema to help the model
        if tool_name in tools_map:
            tool_schema = tools_map[tool_name]
            input_schema = tool_schema.get("inputSchema", {})
            required_params = input_schema.get("required", [])
            properties = input_schema.get("properties", {})
            
            if required_params:
                error_lines.append(f"\nâœ… REQUIRED Parameters for '{tool_name}':")
                for param in required_params:
                    param_schema = properties.get(param, {})
                    param_type = param_schema.get("type", "unknown")
                    param_desc = param_schema.get("description", "No description")
                    error_lines.append(f"  â€¢ {param} ({param_type}): {param_desc}")
            
            # Show optional parameters too
            optional_params = [p for p in properties.keys() if p not in required_params]
            if optional_params:
                error_lines.append(f"\nðŸ“‹ Optional Parameters:")
                for param in optional_params:
                    param_schema = properties.get(param, {})
                    param_type = param_schema.get("type", "unknown")
                    param_desc = param_schema.get("description", "No description")
                    error_lines.append(f"  â€¢ {param} ({param_type}): {param_desc}")
        
        error_lines.append("")  # Blank line between errors
    
    # Create example with actual first failed tool name for clarity
    example_tool_name = validation_errors[0].get("tool_name", "tool_name") if validation_errors else "tool_name"
    example_params = '        "param_name": "..."'
    
    error_lines.extend([
        "=" * 80,
        "INSTRUCTIONS:",
        f"Generate {len(validation_errors)} corrected tool call(s) - one for each failed tool above.",
        "Provide the REQUIRED parameters with appropriate values based on the context.",
        "The valid tool calls from your previous attempt will be kept automatically.",
        "",
        "Example corrected response format:",
        "{",
        '  "reasoning": "...",',
        '  "tool_calls": [',
        '    {',
        f'      "tool_name": "{example_tool_name}",',
        '      "parameters": {',
        example_params,
        '      },',
        '      "reasoning": "..."',
        '    }',
        '  ]',
        "}",
        "=" * 80,
        ""
    ])
    
    return "\n".join(error_lines)


def _build_context_from_hops(hops_array: List[Dict[str, Any]], state: State) -> Dict[str, Any]:
    """Build context from previous hops for planning."""
    # Current hop is the next one to plan (len of completed hops + 1)
    current_hop = len(hops_array) + 1
    
    context = {
        "timestamp": state.get("timestamp"),
        "current_hop": current_hop,
        "max_hops": state.get("max_hops", 3),
        "tool_executions": [],  # List of {tool_name, parameters, success, error}
        "doc_searches": [],  # List of {query, result_count, success}
        "coverage_analysis": None
    }
    
    # Aggregate data from all previous hops
    for hop in hops_array:
        # Get plan data to extract tool calls with parameters
        plan_data = hop.get("plan", {})
        tool_calls_from_plan = plan_data.get("tool_calls", [])
        
        # Get gather data to see results
        gather_data = hop.get("gather", {})
        if gather_data:
            tool_results = gather_data.get("tool_results", [])
            
            # Match tool results with their plan by index (order matters)
            for idx, result in enumerate(tool_results):
                tool_name = result.get("tool_name", "unknown")
                success = result.get("success", False)
                error = result.get("error")
                
                # Get parameters from the corresponding tool call by index
                parameters = {}
                if idx < len(tool_calls_from_plan):
                    tool_call = tool_calls_from_plan[idx]
                    # Verify tool names match (sanity check)
                    if tool_call.get("tool_name") == tool_name:
                        parameters = tool_call.get("parameters", {})
                
                # Special handling for doc searches
                if tool_name == "search_talent_docs":
                    query = parameters.get("query", "unknown query")
                    result_count = 0
                    if success and result.get("data"):
                        # Parse result data to get count
                        data_list = result.get("data", [])
                        if data_list and isinstance(data_list, list):
                            for item in data_list:
                                if isinstance(item, dict) and item.get("type") == "text":
                                    try:
                                        import json
                                        parsed = json.loads(item.get("text", "{}"))
                                        result_count = parsed.get("total_results", 0)
                                        break
                                    except:
                                        pass
                    
                    context["doc_searches"].append({
                        "query": query,
                        "result_count": result_count,
                        "success": success
                    })
                else:
                    # Regular tool execution
                    context["tool_executions"].append({
                        "tool_name": tool_name,
                        "parameters": parameters,
                        "success": success,
                        "error": error if not success else None
                    })
        
        # Get coverage data (will be overwritten by latest hop)
        coverage_data = hop.get("coverage", {})
        if coverage_data and coverage_data.get("coverage_response"):
            # Always overwrite with latest hop's coverage analysis (only reasoning will be used)
            context["coverage_analysis"] = coverage_data["coverage_response"]
    
    return context




def _format_context_for_prompt(context: Dict[str, Any]) -> str:
    """Format context information for the LLM prompt."""
    if not context:
        return "No previous context available"
    
    context_parts = []
    
    # Hop information
    current_hop = context.get("current_hop", 1)
    max_hops = context.get("max_hops", 3)
    context_parts.append(f"- Planning for hop: {current_hop}/{max_hops}")
    
    # Previous tool executions with signatures
    tool_executions = context.get("tool_executions", [])
    if tool_executions:
        context_parts.append("\n- Previously executed tools:")
        for execution in tool_executions:
            tool_name = execution.get("tool_name", "unknown")
            params = execution.get("parameters", {})
            success = execution.get("success", False)
            error = execution.get("error")
            
            # Format parameters compactly
            param_str = ", ".join([f"{k}={repr(v)}" for k, v in params.items()])
            status = "âœ“ SUCCESS" if success else f"âœ— FAILED ({error})"
            context_parts.append(f"  * {tool_name}({param_str}) - {status}")
    
    # Previous doc searches with result counts
    doc_searches = context.get("doc_searches", [])
    if doc_searches:
        context_parts.append("\n- Previously searched documentation:")
        for search in doc_searches:
            query = search.get("query", "unknown")
            result_count = search.get("result_count", 0)
            success = search.get("success", False)
            status = f"{result_count} results" if success else "FAILED"
            context_parts.append(f"  * '{query}' - {status}")
    
    # Coverage analysis results (reasoning and missing data from latest hop)
    coverage_analysis = context.get("coverage_analysis")
    if coverage_analysis:
        reasoning = coverage_analysis.get('reasoning', '')
        if reasoning:
            context_parts.append(f"\n- Coverage analysis from previous hop: {reasoning}")
        
        # Include missing data gaps to guide tool selection
        missing_data = coverage_analysis.get('missing_data', [])
        if missing_data:
            context_parts.append(f"\n- Missing data identified by coverage:")
            for gap in missing_data:
                gap_type = gap.get('gap_type', 'unknown')
                description = gap.get('description', '')
                context_parts.append(f"  * {gap_type}: {description}")
    
    # Available docs
    available_docs = context.get("available_docs", [])
    if available_docs:
        context_parts.append(f"\n- Available documentation collected: {len(available_docs)} searches")
    
    return "\n".join(context_parts) if context_parts else "No relevant context available"


def _extract_email_from_query(query: str) -> str:
    """Extract email from user query if present."""
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    match = re.search(email_pattern, query)
    return match.group() if match else None
